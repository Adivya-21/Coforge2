import json
import random
from datetime import datetime, timedelta
from pathlib import Path

OUT_FILE = "employees_1m.jsonl"
N = 1_000_000

FIRST_NAMES = ["Raj", "Asha", "Nisha", "Amit", "Vikram", "Meena", "Anil", "Pooja", "Ramesh", "Kiran"]
DEPTS = [
    {"Name": "IT", "Location": "Delhi"},
    {"Name": "HR", "Location": "Mumbai"},
    {"Name": "Finance", "Location": "Bengaluru"},
    {"Name": "Sales", "Location": "Pune"},
    {"Name": "Ops", "Location": "Hyderabad"},
]

def rand_date(start_year=2018, end_year=2025):
    start = datetime(start_year, 1, 1)
    end   = datetime(end_year, 12, 31)
    delta = end - start
    return (start + timedelta(days=random.randint(0, delta.days))).strftime("%Y-%m-%d")

def make_projects(emp_id):
    k = random.randint(0, 3)  # 0-3 projects
    projects = []
    for i in range(k):
        pid = emp_id * 10_000 + i
        projects.append({"ProjectID": pid, "Title": random.choice(["Migration", "Automation", "Analytics", "Upgrade"])})
    return projects

def main():
    Path(OUT_FILE).unlink(missing_ok=True)
    with open(OUT_FILE, "w", encoding="utf-8") as f:
        for emp_id in range(1, N + 1):
            rec = {
                "EmpID": emp_id,
                "Name": random.choice(FIRST_NAMES),
                "Dept": random.choice(DEPTS),
                "Projects": make_projects(emp_id),
                "Salary": random.randint(20000, 150000),
                "HiredAt": rand_date()
            }
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            if emp_id % 100_000 == 0:
                print(f"[GEN] wrote {emp_id:,} records")
    print(f"Done. File: {OUT_FILE}")

if __name__ == "__main__":
    main()

-------------------------------------------------------------------------------------
# Flattening & placing in MS SQL server 
import json
import pandas as pd
from sqlalchemy import create_engine
from pandas import json_normalize

# ---------- Configure ----------
JSONL_FILE = "employees_1m.jsonl"
CHUNK = 50_000  # tune based on memory/IO
MSSQL_CONN = (
    "mssql+pyodbc://@localhost/ETLDB?"
    "driver=ODBC+Driver+17+for+SQL+Server;Trusted_Connection=yes"
)
EMP_TABLE = "dbo.Employees"
PRJ_TABLE = "dbo.Projects"
# -------------------------------

engine = create_engine(MSSQL_CONN, fast_executemany=True)

def process_batch(lines):
    """
    lines: list[str] of NDJSON lines
    returns: (df_emp, df_prj)
    """
    records = [json.loads(l) for l in lines]

    # Flatten top-level and 'Dept' dict into columns Dept_Name, Dept_Location
    df_emp = json_normalize(records, sep="_")
    # Keep only employee columns we care about
    df_emp = df_emp[["EmpID", "Name", "Dept_Name", "Dept_Location", "Salary", "HiredAt"]]

    # Flatten 'Projects' (list) into separate table, keep EmpID as FK
    df_prj = json_normalize(
        records,
        record_path="Projects",
        meta=["EmpID"],
        sep="_"
    )

    # If no projects existed in this batch, ensure proper columns
    if df_prj.empty:
        df_prj = pd.DataFrame(columns=["ProjectID", "Title", "EmpID"])

    # Ensure dtypes
    df_emp["EmpID"] = df_emp["EmpID"].astype("int64")
    df_emp["Salary"] = pd.to_numeric(df_emp["Salary"], errors="coerce").astype("Int64")
    df_emp["HiredAt"] = pd.to_datetime(df_emp["HiredAt"], errors="coerce").dt.date

    if not df_prj.empty:
        df_prj["EmpID"] = df_prj["EmpID"].astype("int64")
        df_prj["ProjectID"] = pd.to_numeric(df_prj["ProjectID"], errors="coerce").astype("Int64")

    return df_emp, df_prj

def main():
    total = 0
    with open(JSONL_FILE, "r", encoding="utf-8") as f:
        batch = []
        for line in f:
            if line.strip():
                batch.append(line)
            if len(batch) >= CHUNK:
                df_emp, df_prj = process_batch(batch)
                df_emp.to_sql(EMP_TABLE, engine, if_exists="append", index=False)
                if not df_prj.empty:
                    df_prj.to_sql(PRJ_TABLE, engine, if_exists="append", index=False)
                total += len(batch)
                print(f"[LOAD] inserted {total:,} rows (employees) so far")
                batch.clear()

        # Flush remainder
        if batch:
            df_emp, df_prj = process_batch(batch)
            df_emp.to_sql(EMP_TABLE, engine, if_exists="append", index=False)
            if not df_prj.empty:
                df_prj.to_sql(PRJ_TABLE, engine, if_exists="append", index=False)
            total += len(batch)
            print(f"[LOAD] inserted {total:,} rows total")

    print("Done.")

if __name__ == "__main__":
    main()

-------------------------------------------------------------------

import json
import pandas as pd
from sqlalchemy import create_engine
from pandas import json_normalize
import logging
import urllib

# Configure logging for better visibility into the process
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---------- Configure ----------
JSONL_FILE = "employees_1m.jsonl"
CHUNK_SIZE_LINES = 50_000

MSSQL_CONN = (
    "mssql+pyodbc:///?odbc_connect="
    "Driver={ODBC+Driver+17+for+SQL+Server};"
    "Server=localhost\\SQLEXPRESS;"
    "Database=ETLDB;"
    "Trusted_Connection=yes;"
)

engine = create_engine(MSSQL_CONN, fast_executemany=True)

# Correct the table names to exclude the schema prefix
EMP_TABLE = "Employees"
PRJ_TABLE = "Projects"
DB_SCHEMA = "dbo"  # Specify the schema separately
# -------------------------------

def process_batch(lines):
    # This function remains the same as in your previous script
    records = []
    for l in lines:
        try:
            records.append(json.loads(l))
        except json.JSONDecodeError as e:
            logging.error(f"Skipping malformed JSON line: {l.strip()}. Error: {e}")
            continue

    if not records:
        return pd.DataFrame(), pd.DataFrame()

    df_emp = json_normalize(records, sep="_")
    df_prj = json_normalize(records, record_path="Projects", meta=["EmpID"], sep="_", errors="ignore")

    emp_cols = ["EmpID", "Name", "Dept_Name", "Dept_Location", "Salary", "HiredAt"]
    df_emp = df_emp[[c for c in emp_cols if c in df_emp.columns]]
    df_emp["EmpID"] = pd.to_numeric(df_emp["EmpID"], errors="coerce").astype("Int64")
    df_emp["Salary"] = pd.to_numeric(df_emp["Salary"], errors="coerce").astype("Int64")
    df_emp["HiredAt"] = pd.to_datetime(df_emp["HiredAt"], errors="coerce").dt.date

    if not df_prj.empty:
        prj_cols = ["ProjectID", "Title", "EmpID"]
        df_prj = df_prj[[c for c in prj_cols if c in df_prj.columns]]
        df_prj["EmpID"] = pd.to_numeric(df_prj["EmpID"], errors="coerce").astype("Int64")
        df_prj["ProjectID"] = pd.to_numeric(df_prj["ProjectID"], errors="coerce").astype("Int64")
    else:
        df_prj = pd.DataFrame(columns=["ProjectID", "Title", "EmpID"])

    return df_emp, df_prj

def main():
    total_emp = 0
    total_prj = 0
    emp_cols_count = 6
    prj_cols_count = 3
    insert_chunksize_emp = 2100 // emp_cols_count - 1
    insert_chunksize_prj = 2100 // prj_cols_count - 1

    logging.info(f"Using database chunksize: {insert_chunksize_emp} for employees, {insert_chunksize_prj} for projects")

    try:
        with open(JSONL_FILE, "r", encoding="utf-8") as f:
            batch = []
            for line in f:
                if line.strip():
                    batch.append(line)
                if len(batch) >= CHUNK_SIZE_LINES:
                    df_emp, df_prj = process_batch(batch)
                   
                    if not df_emp.empty:
                        df_emp.to_sql(
                            EMP_TABLE,
                            engine,
                            if_exists="append",
                            index=False,
                            method='multi',
                            chunksize=insert_chunksize_emp,
                            schema=DB_SCHEMA  # Pass schema explicitly
                        )
                        total_emp += len(df_emp)
                        logging.info(f"Inserted {total_emp:,} employee rows so far")

                    if not df_prj.empty:
                        df_prj.to_sql(
                            PRJ_TABLE,
                            engine,
                            if_exists="append",
                            index=False,
                            method='multi',
                            chunksize=insert_chunksize_prj,
                            schema=DB_SCHEMA # Pass schema explicitly
                        )
                        total_prj += len(df_prj)
                        logging.info(f"Inserted {total_prj:,} project rows so far")

                    batch.clear()

            if batch:
                df_emp, df_prj = process_batch(batch)
                if not df_emp.empty:
                    df_emp.to_sql(
                        EMP_TABLE,
                        engine,
                        if_exists="append",
                        index=False,
                        method='multi',
                        chunksize=insert_chunksize_emp,
                        schema=DB_SCHEMA
                    )
                    total_emp += len(df_emp)
               
                if not df_prj.empty:
                    df_prj.to_sql(
                        PRJ_TABLE,
                        engine,
                        if_exists="append",
                        index=False,
                        method='multi',
                        chunksize=insert_chunksize_prj,
                        schema=DB_SCHEMA
                    )
                    total_prj += len(df_prj)

    except Exception as e:
        logging.error(f"An error occurred during file processing: {e}")
   
    logging.info("--- Data loading complete ---")
    logging.info(f"Total employee rows inserted: {total_emp:,}")
    logging.info(f"Total project rows inserted: {total_prj:,}")

if __name__ == "__main__":
    main()
----------------------------------------------

create Database ETLDB;
use ETLDB;
create Table Employees ( EmpID bigint, Name varchar(50), Dept_Name varchar(50), Dept_Location varchar(50), Salary int, HiredAt date);

create Table Projects (ProjectID int, Title varchar(100), EmpID bigint);

