Step 1: Prepare Project Directory
Choose a working folder (example: airflow_project):
mkdir airflow_project && cd airflow_project
Inside it we will create:
airflow_project/
├── dags/                  # your DAG files go here
│    └── example_dag.py     # sample DAG
├── spark_jobs/            # PySpark ETL scripts
│       └── etl_yellow_taxi.py
├── mysql_data/            # volume mount for MySQL database files
├── docker-compose.yaml    # main docker compose file
├── .env                   # environment variables (Airflow + MySQL)
└── requirements.txt       # optional, extra python packages for Airflow
 You must create all these directories and files.

Step 2: Create .env
airflow_project/.env
# -------------------------
# Airflow Config
# -------------------------
AIRFLOW_UID=50000
AIRFLOW_GID=0

# Generate with:
# python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY_HERE

# -------------------------
# Airflow Admin User
# -------------------------
_AIRFLOW_WWW_USER_USERNAME=airflow
_AIRFLOW_WWW_USER_PASSWORD=airflow

# -------------------------
# MySQL Config (used by Airflow)
# -------------------------
MYSQL_HOST=mysql
MYSQL_PORT=3306
MYSQL_USER=airflow
MYSQL_PASSWORD=airflow
MYSQL_DB=airflow_db

 Step 3: Create docker-compose.yaml
airflow_project/docker-compose.yaml
version: "3.9"

x-airflow-common: &airflow-common
  image: apache/airflow:2.8.1
  env_file:
    - .env
  volumes:
    - ./dags:/opt/airflow/dags
    - ./spark_jobs:/opt/airflow/spark_jobs
    - ./requirements.txt:/requirements.txt
  user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
  depends_on:
    - mysql

services:
  mysql:
    image: mysql:8.0
    container_name: mysql_airflow
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: ${MYSQL_DB}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"
    volumes:
      - ./mysql_data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      retries: 5

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install --no-cache-dir -r /requirements.txt
        airflow db init
        airflow users create \
          --username ${_AIRFLOW_WWW_USER_USERNAME} \
          --firstname Admin --lastname User \
          --role Admin \
          --email admin@example.com \
          --password ${_AIRFLOW_WWW_USER_PASSWORD}

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler

 Step 4: Create requirements.txt
For Spark + MySQL support:
airflow_project/requirements.txt
pyspark
pandas
mysql-connector-python
sqlalchemy

Step 5: Create a Sample DAG
airflow_project/dags/example_dag.py
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from datetime import timedelta

def notify_success(**kwargs):
    print("DAG executed successfully!")

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}

with DAG(
    dag_id="example_mysql_dag",
    default_args=default_args,
    description="Simple DAG to test Airflow + MySQL setup",
    schedule_interval="@daily",
    start_date=days_ago(1),
    catchup=False,
) as dag:

    t1 = BashOperator(
        task_id="print_date",
        bash_command="date"
    )

    t2 = PythonOperator(
        task_id="notify",
        python_callable=notify_success,
    )

    t1 >> t2

 Step 6: Initialize and Start Airflow
1.	Start MySQL + init DB:
2.	docker-compose up airflow-init
This will initialize the Airflow metadata DB in MySQL and create the admin user.
3.	Bring up all services:
4.	docker-compose up -d
5.	Access Airflow UI:
o	URL: http://localhost:8080
o	User: airflow
o	Pass: airflow

Step 7: Verify MySQL Connection
Inside Airflow container:
docker exec -it airflow_project-airflow-webserver-1 bash
airflow db check



