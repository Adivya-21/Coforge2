import requests
import pandas as pd
import numpy as np
import time
from sqlalchemy import create_engine, text

# API SETUP -------------------
url = "https://api.coingecko.com/api/v3/coins/markets"
params = {
    "vs_currency": "usd",
    "order": "market_cap_desc",
    "per_page": 250,   # max per request
    "page": 1,
    "sparkline": False
}

total_pages = 4000       # ~1M rows
chunk_size = 20          # 20 pages (~5k rows) per chunk
sleep_time = 2.5         # safer than 1.2s
max_retries = 5          # retry API failures
time_limit = 600         # stop after 10 min
start_time = time.time()

# Database connection (MS SQL)
mssql_engine = create_engine(
    "mssql+pyodbc:///?odbc_connect="
    "Driver={ODBC Driver 17 for SQL Server};"
    "Server=DESKTOP-6B6KT5K;"
    "Database=cryptodb;"
    "Trusted_Connection=yes;"
)

def fetch_page(page):
    """Fetch a single page with retries"""
    for attempt in range(max_retries):
        try:
            params["page"] = page
            response = requests.get(url, params=params, timeout=15)

            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:  # Too many requests
                wait_time = (attempt + 1) * 10
                print(f"429 Too Many Requests. Sleeping {wait_time}s before retry...")
                time.sleep(wait_time)
            else:
                print(f"Failed page {page}, status {response.status_code}")
                return []
        except Exception as e:
            print(f"Error on page {page}, attempt {attempt+1}: {e}")
            time.sleep((attempt + 1) * 5)

    print(f"Skipping page {page} after {max_retries} retries")
    return []


for start_page in range(1, total_pages + 1, chunk_size):
    all_data = []

    # Stop if time limit exceeded
    if time.time() - start_time > time_limit:
        print("Time limit reached, stopping extraction.")
        break

    # Fetch a chunk
    for page in range(start_page, start_page + chunk_size):
        if page > total_pages:
            break

        page_data = fetch_page(page)
        if page_data:
            all_data.extend(page_data)
            print(f"Page {page} fetched ({len(page_data)} rows,
                  chunk size {len(all_data)})")

        time.sleep(sleep_time)  # throttle requests

    # Convert JSON → DataFrame
    df = pd.json_normalize(all_data)
    if df.empty:
        continue

    # TRANSFORMATION -------------------
    df = df.drop_duplicates(subset="id")
    df["price_log"] = df["current_price"].apply(lambda x: np.log(x) if x and x > 0 else None)
    if df["total_volume"].std() > 0:
        df["volume_normalized"] = (df["total_volume"] - df["total_volume"].mean()) / df["total_volume"].std()
    else:
        df["volume_normalized"] = 0

    # LOAD INTO DATABASE -------------------
    if start_page == 1:
        df.to_sql("crypto_data", mssql_engine, if_exists="replace", index=False)
    else:
        df.to_sql("crypto_data", mssql_engine, if_exists="append", index=False)

    print(f"Loaded chunk starting at page {start_page} → {len(df)} rows")

    # Extra pause between chunks
    time.sleep(10)

# VERIFY -------------------
with mssql_engine.connect() as conn:
    result = conn.execute(text("SELECT COUNT(*) FROM crypto_data"))
    print("Final row count in DB:", result.scalar())
